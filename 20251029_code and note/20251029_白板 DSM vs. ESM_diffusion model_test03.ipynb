{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab28308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# ESM vs DSM on 1D Gaussian: x ~ N(3, sigma_data^2)\n",
    "# 目標：在「白板例子」上驗證 ESM 與 DSM 的對齊關係\n",
    "# =========================================================\n",
    "import math, random, numpy as np\n",
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(0); np.random.seed(0); random.seed(0)\n",
    "device = torch.device(\"cpu\")  # 如有 GPU: torch.device(\"cuda\")\n",
    "\n",
    "# ----------------------------\n",
    "# Data: 1D Gaussian  N(3, sigma_data^2)\n",
    "# ----------------------------\n",
    "sigma_data = 1.0   # 白板常用 1（你要改也行）\n",
    "mu_data    = 3.0\n",
    "\n",
    "def sample_data(n):\n",
    "    return mu_data + sigma_data * torch.randn(n, 1, device=device)\n",
    "\n",
    "def true_score(x):\n",
    "    # s(x) = d/dx log N(3, sigma_data^2) = -(x-3)/sigma_data^2\n",
    "    return -(x - mu_data) / (sigma_data**2)\n",
    "\n",
    "# ----------------------------\n",
    "# Model: tiny MLP score(x, [log σ])\n",
    "# ----------------------------\n",
    "class Score1D(nn.Module):\n",
    "    def __init__(self, hidden=32, use_sigma=False):\n",
    "        super().__init__()\n",
    "        self.use_sigma = use_sigma\n",
    "        in_dim = 1 + (1 if use_sigma else 0)\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden), nn.SiLU(),\n",
    "            nn.Linear(hidden, hidden), nn.SiLU(),\n",
    "            nn.Linear(hidden, 1),\n",
    "        )\n",
    "        for m in self.net:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight); nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x, sigma=None):\n",
    "        if self.use_sigma and (sigma is not None):\n",
    "            if not torch.is_tensor(sigma):\n",
    "                sigma = torch.tensor(float(sigma), dtype=x.dtype, device=x.device)\n",
    "            h = torch.cat([x, torch.log(sigma).expand_as(x)], dim=1)\n",
    "        else:\n",
    "            h = x\n",
    "        return self.net(h)\n",
    "\n",
    "# ----------------------------\n",
    "# Losses\n",
    "# ----------------------------\n",
    "def esm_loss(model, n_batch=1024):\n",
    "    \"\"\"\n",
    "    Exact（有真分數可用）：\n",
    "    L_ESM = E_x [ || s_theta(x) - s_true(x) ||^2 ]\n",
    "    這裡不需要 Hutchinson，因為白板案例已知真 score。\n",
    "    \"\"\"\n",
    "    x = sample_data(n_batch)\n",
    "    s_pred = model(x, sigma=None)\n",
    "    s_true = true_score(x)\n",
    "    return ((s_pred - s_true)**2).mean()\n",
    "\n",
    "def dsm_loss_fixed_sigma(model, sigma_noise=0.2, n_batch=1024, lambda_by_sigma=True, clip=50.0):\n",
    "    \"\"\"\n",
    "    y = x0 + sigma_noise*z,  z~N(0,1)\n",
    "    target = (x0 - y)/sigma^2\n",
    "    \"\"\"\n",
    "    x0 = sample_data(n_batch)\n",
    "    z  = torch.randn_like(x0)\n",
    "    y  = x0 + sigma_noise * z\n",
    "    target = (x0 - y) / (sigma_noise**2)\n",
    "    if clip is not None:\n",
    "        target = torch.clamp(target, -clip, clip)\n",
    "    s_pred = model(y, sigma=sigma_noise)\n",
    "    per = (s_pred - target)**2\n",
    "    if lambda_by_sigma:\n",
    "        per = (sigma_noise**2) * per  # 常見 reweight\n",
    "    return per.mean()\n",
    "\n",
    "def dsm_loss_multisigma(model, n_batch=1024, low=5e-3, high=0.2, lambda_rule=\"sigma2\", clip=50.0):\n",
    "    \"\"\"\n",
    "    多層級噪聲（log-uniform）：σ ~ LogU(low, high)\n",
    "    \"\"\"\n",
    "    x0 = sample_data(n_batch)\n",
    "    u = torch.rand(n_batch, 1, device=device)\n",
    "    sigmas = torch.exp(u * (math.log(high) - math.log(low)) + math.log(low))  # (B,1)\n",
    "    z  = torch.randn_like(x0)\n",
    "    y  = x0 + sigmas * z\n",
    "    target = (x0 - y) / (sigmas**2)\n",
    "    if clip is not None:\n",
    "        target = torch.clamp(target, -clip, clip)\n",
    "    s_pred = model(y, sigma=sigmas)\n",
    "    per = (s_pred - target)**2\n",
    "    if lambda_rule == \"sigma2\":\n",
    "        per = (sigmas**2) * per\n",
    "    return per.mean()\n",
    "\n",
    "# ----------------------------\n",
    "# Train loops（會印每步 loss）\n",
    "# ----------------------------\n",
    "def train_loop(model, loss_fn, steps=200, batch=1024, lr=1e-3, tag=\"ESM\"):\n",
    "    model.train()\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    hist = []\n",
    "    for t in range(1, steps+1):\n",
    "        loss = loss_fn(model, n_batch=batch)\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        opt.step()\n",
    "        hist.append(loss.item())\n",
    "        if t % 20 == 0:\n",
    "            print(f\"[{tag} {t:04d}] loss={loss.item():.6f}, lr={opt.param_groups[0]['lr']:.2e}\")\n",
    "    return np.array(hist, dtype=float)\n",
    "\n",
    "# ----------------------------\n",
    "# 實驗：白板版 ESM vs DSM\n",
    "# ----------------------------\n",
    "STEPS, BATCH = 200, 1024\n",
    "\n",
    "# ESM：學 s(x) ≈ -(x-3)/sigma_data^2\n",
    "esm_net = Score1D(hidden=32, use_sigma=False).to(device)\n",
    "esm_curve = train_loop(\n",
    "    esm_net,\n",
    "    loss_fn=lambda M, n_batch: esm_loss(M, n_batch),\n",
    "    steps=STEPS, batch=BATCH, lr=1e-3, tag=\"ESM\"\n",
    ")\n",
    "\n",
    "# DSM（多 σ，前期不 reweight → 後期用 σ^2）\n",
    "dsm_net = Score1D(hidden=32, use_sigma=True).to(device)\n",
    "dsm_curve = []\n",
    "for t in range(1, STEPS+1):\n",
    "    rule = \"none\" if t <= 120 else \"sigma2\"\n",
    "    loss = dsm_loss_multisigma(dsm_net, n_batch=BATCH, low=5e-3, high=0.2,\n",
    "                               lambda_rule=rule, clip=50.0)\n",
    "    if t == 1:\n",
    "        opt = torch.optim.Adam(dsm_net.parameters(), lr=3e-3)\n",
    "    opt.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(dsm_net.parameters(), 1.0)\n",
    "    opt.step()\n",
    "    dsm_curve.append(loss.item())\n",
    "    if t % 20 == 0:\n",
    "        print(f\"[DSM  {t:04d}] loss={loss.item():.6f}, rule={rule}, lr={opt.param_groups[0]['lr']:.2e}\")\n",
    "dsm_curve = np.array(dsm_curve, dtype=float)\n",
    "\n",
    "# ----------------------------\n",
    "# Cross-evaluation（在白板例子上檢查「接近」）\n",
    "# ----------------------------\n",
    "with torch.no_grad():\n",
    "    X_eval = sample_data(4000)\n",
    "\n",
    "# 1) ESM native\n",
    "native_esm = ((esm_net(X_eval) - true_score(X_eval))**2).mean().item()\n",
    "\n",
    "# 2) DSM native（在 σ→小 的 regime）\n",
    "def dsm_eval_near_zero(model, x, sigmas=(1e-2, 2e-2, 5e-2)):\n",
    "    vals = []\n",
    "    for s in sigmas:\n",
    "        z = torch.randn_like(x)\n",
    "        y = x + s * z\n",
    "        target = (x - y) / (s*s)\n",
    "        pred = model(y, sigma=s)\n",
    "        vals.append(((pred - target)**2).mean().item())\n",
    "    return float(sum(vals)/len(vals))\n",
    "\n",
    "native_dsm_small = dsm_eval_near_zero(dsm_net, X_eval)\n",
    "\n",
    "# 3) DSM 模型的 ESM（用很小 σ，期望接近）\n",
    "def esm_of_dsm(model, x, tiny_sigma=1e-2):\n",
    "    # 用 DSM 模型在 tiny σ 下的輸出近似 s(x)\n",
    "    s_pred = model(x, sigma=tiny_sigma)\n",
    "    return ((s_pred - true_score(x))**2).mean().item()\n",
    "\n",
    "cross_esm_on_dsm = esm_of_dsm(dsm_net, X_eval, tiny_sigma=1e-2)\n",
    "\n",
    "# 4) ESM 模型的 DSM（小 σ）\n",
    "def dsm_of_esm(model, x, sigmas=(1e-2, 2e-2, 5e-2)):\n",
    "    vals = []\n",
    "    for s in sigmas:\n",
    "        z = torch.randn_like(x)\n",
    "        y = x + s * z\n",
    "        target = (x - y) / (s*s)\n",
    "        pred = model(y, sigma=None)  # ESM 模型不吃 σ\n",
    "        vals.append(((pred - target)**2).mean().item())\n",
    "    return float(sum(vals)/len(vals))\n",
    "\n",
    "cross_dsm_on_esm = dsm_of_esm(esm_net, X_eval)\n",
    "\n",
    "print(\"\\n=== Evaluation on 1D Gaussian ===\")\n",
    "print(f\"Native ESM (to true score):        {native_esm:.6f}\")\n",
    "print(f\"Native DSM (σ in { (1e-2,2e-2,5e-2) }): {native_dsm_small:.6f}\")\n",
    "print(f\"ESM of DSM-model (σ≈1e-2):         {cross_esm_on_dsm:.6f}\")\n",
    "print(f\"DSM of ESM-model (avg small σ):    {cross_dsm_on_esm:.6f}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 圖：訓練曲線\n",
    "# ----------------------------\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(esm_curve, label=\"ESM train loss\")\n",
    "plt.plot(dsm_curve, label=\"DSM train loss (multi-σ)\")\n",
    "plt.xlabel(\"Step\"); plt.ylabel(\"Loss\"); plt.title(\"Training curves on 1D Gaussian\")\n",
    "plt.legend(); plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3591ed37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90a327a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
