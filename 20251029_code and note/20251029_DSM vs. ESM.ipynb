{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3a95c69",
   "metadata": {},
   "source": [
    "# 比較 DSM loss 跟 ESM loss\n",
    "\n",
    "* ***DSM Loss***: 先加噪 $y=x+\\sigma z,\\,\\, z\\sim N(0,I)$, 再最小化 $$L_{DSM}(\\theta)=\\mathbb{E}[\\,\\lambda(\\sigma)\\,\\, \\big||s_{\\theta}(y,\\sigma)-\\frac{x-y}{\\sigma}\\big||^2\\,],$$ 其中這邊用$\\lambda(\\sigma)=\\sigma^2$\n",
    "* ***ESM Loss***: $$L_{ESM}(\\theta)=\\mathbb{E}_{x\\sim p}[\\frac{1}{2}||s_{\\theta}(x)||^2+\\text{div}_{x}s_{\\theta}(x)]$$ 這邊的散度定義div為 $\\text{tr}(J)=\\mathbb{E}_v[v^{T}Jv]$, 又稱 **Hutchinson’s trick**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964a8b5e",
   "metadata": {},
   "source": [
    "## DATASET\n",
    "* 產生2D的 normal distribution 點\n",
    "  * 中心 $(0,0)$; variance matrix $\\begin{bmatrix}1 & 0.3\\\\ 0.3 & 1.2\\end{bmatrix}$\n",
    "  * 中心 $(3,3)$; variance matrix $\\begin{bmatrix}0.6 & -0.2\\\\ -0.2 & 0.8\\end{bmatrix}$\n",
    "* 建立最小的MLP score net\n",
    "* 計算ESM 跟DSM 的一次性數值(不訓練)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcdabdb",
   "metadata": {},
   "source": [
    "## Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acc0d8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, math, random, numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53cf38bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "412a07ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_gmm(n=8192):\n",
    "    n1 = n//2\n",
    "    n2 = n-n1\n",
    "    mean1 = torch.tensor([0.0,0.0])\n",
    "    mean2 = torch.tensor([3.3,3.3])\n",
    "    cov1 = torch.tensor([[1.0, 0.3],\n",
    "                         [0.3, 1.2]], dtype=torch.float32)\n",
    "    cov2 = torch.tensor([[0.6, -0.2],\n",
    "                         [-0.2, 0.8]], dtype=torch.float32)\n",
    "    L1 = torch.linalg.cholesky(cov1)\n",
    "    L2 = torch.linalg.cholesky(cov2)\n",
    "    z1 = torch.randn(n1,2) @ L1.T +mean1\n",
    "    z2 = torch.randn(n2,2) @ L2.T +mean2\n",
    "    \n",
    "    x = torch.cat([z1, z2], dim=0)\n",
    "    return x.to(device)\n",
    "\n",
    "X=sample_gmm(4096). requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f1b1c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScoreMLP(nn.Module):\n",
    "    def __init__(self, in_dim, hidden, use_sigma=True):\n",
    "        super().__init__()\n",
    "        self.use_sigma = use_sigma \n",
    "        eff_in = in_dim + (1 if use_sigma else 0)\n",
    "        self.fc1 = nn.Linear(eff_in, hidden)\n",
    "        self.fc2 = nn.Linear(hidden, hidden)\n",
    "        self.fc3 = nn.Linear(hidden, in_dim)\n",
    "\n",
    "        for m in [self.fc1, self.fc2, self.fc3]:\n",
    "            nn.init.xavier_uniform_(m.weight, gain=1.0)\n",
    "            nn.init.zeros_(m.bias)\n",
    "    \n",
    "    def forward(self, x, sigma=None):\n",
    "        if self.use_sigma and (sigma is not None):\n",
    "            if not torch.is_tensor(sigma):\n",
    "                sigma = torch.tensor(sigma, dtype=x.dtype, device=x.device)\n",
    "            log_sigma = torch.log(sigma).expand(x.shape[0], 1)\n",
    "            h=torch.cat([x, log_sigma], dim=1)\n",
    "        else:\n",
    "            h=x\n",
    "        h=F.silu(self.fc1(h))\n",
    "        h=F.silu(self.fc2(h))\n",
    "        out =self.fc3(h)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "model_ESM = ScoreMLP(in_dim=2, hidden=64, use_sigma = False).to(device)\n",
    "model_DSM = ScoreMLP(in_dim=2, hidden=64, use_sigma = True).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7fe8d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ESM_loss(model, x, num_trace_samples=1):\n",
    "    x = x.requires_grad_(True)\n",
    "    s = model(x, sigma=None)\n",
    "    sq = 0.5*(s*s).sum(dim=1)\n",
    "    div_est_total = torch.zeros_like(sq)\n",
    "\n",
    "    for _ in range(num_trace_samples):\n",
    "        v = torch.empty_like(x).bernoulli_(0.5).mul_(2).sub(1) # Redemacher??\n",
    "        sv = (s*v).sum()\n",
    "        (grad_x,) = torch.autograd.gard(sv, x, vreate_graph = True)\n",
    "        trJ_est = (grad_x * v).sum(dim=1)\n",
    "        div_est_total = div_est_total+trJ_est\n",
    "    div_est = div_est_total / float(num_trace_samples)\n",
    "    \n",
    "    return (sq+div_est).mean()\n",
    "\n",
    "def DSM_loss(model, x, sigma=0.2, lambda_by_sigma=True):\n",
    "    sigma = torch.tensor(float(sigma), device = x.device)\n",
    "    z= torch.randn_like(x)\n",
    "    y=x+sigma *z\n",
    "    target =(x-y)/(sigma*sigma)\n",
    "    pred = model(y.detach(), sigma=sigma)\n",
    "    per = (pred-target)\n",
    "    per = (per*per).sum(dim=1)\n",
    "    if lambda_by_sigma:\n",
    "        per = (sigma*sigma)*per\n",
    "    \n",
    "    return per.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbb8f2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
